{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "GiVShlMGwOrT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCGhah0su3gD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "from typing import List, Dict, Union, Set, Any\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict, Counter\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import torchaudio\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaYOiOAezo_1",
        "outputId": "b156ac60-15ae-4e7d-8e7f-5d091518f2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading jiwer-3.0.5-py3-none-any.whl (21 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.5 rapidfuzz-3.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data"
      ],
      "metadata": {
        "id": "KWvtM1owxM1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bb80729"
      },
      "outputs": [],
      "source": [
        "# !pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e9e800e",
        "outputId": "c3f964bd-3a39-4f33-cbf4-f66b534edd58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mfekadu/darpa-timit-acousticphonetic-continuous-speech\n",
            "License(s): copyright-authors\n",
            "Downloading darpa-timit-acousticphonetic-continuous-speech.zip to /content\n",
            " 99% 819M/829M [00:05<00:00, 118MB/s]\n",
            "100% 829M/829M [00:05<00:00, 148MB/s]\n"
          ]
        }
      ],
      "source": [
        "# https://github.com/Kaggle/kaggle-api - Docs kaggle\n",
        "# Simplest way: go to https://www.kaggle.com/settings , \"Create new token\" and move it into \"~/.kaggle\"\n",
        "\n",
        "!kaggle datasets download -d mfekadu/darpa-timit-acousticphonetic-continuous-speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c27ded70"
      },
      "outputs": [],
      "source": [
        "!unzip -o -q darpa-timit-acousticphonetic-continuous-speech.zip -d timit/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "LUg7iCvIyXSY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b82d704a"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "class TimitDataset(Dataset):\n",
        "    \"\"\"Загрузка TIMIT данных с диска\"\"\"\n",
        "    def __init__(self, data_path):\n",
        "        self.data_path = data_path\n",
        "        self.uri2wav = {}\n",
        "        self.uri2text = {}\n",
        "        self.uri2word_ali = {}\n",
        "        self.uri2phone_ali = {}\n",
        "        for d, _, fs in os.walk(data_path):\n",
        "            for f in fs:\n",
        "                full_path = f'{d}/{f}'\n",
        "                if f.endswith('.WAV'):\n",
        "                    # skip it. Use .wav instead\n",
        "                    pass\n",
        "                elif f.endswith('.wav'):\n",
        "                    stem = Path(f[:-4]).stem # .WAV.wav\n",
        "                    self.uri2wav[f'{d}/{stem}'] = full_path\n",
        "                elif f.endswith('.TXT'):\n",
        "                    stem = Path(f).stem\n",
        "                    self.uri2text[f'{d}/{stem}'] = full_path\n",
        "                elif f.endswith('.WRD'):\n",
        "                    stem = Path(f).stem\n",
        "                    self.uri2word_ali[f'{d}/{stem}'] = full_path\n",
        "                elif f.endswith('.PHN'):\n",
        "                    stem = Path(f).stem\n",
        "                    self.uri2phone_ali[f'{d}/{stem}'] = full_path\n",
        "                else:\n",
        "                    warnings.warn(f\"Unknown file type {full_path} . Skip it.\")\n",
        "\n",
        "        self.uris = list(sorted(set(self.uri2wav.keys()) \\\n",
        "                                & set(self.uri2text.keys()) \\\n",
        "                                & set(self.uri2word_ali.keys()) \\\n",
        "                                &  set(self.uri2phone_ali.keys())\n",
        "                               ))\n",
        "        print(f\"Found {len(self.uris)} utterances in {self.data_path}. \",\n",
        "              f\"{len(self.uri2wav)} wavs, \",\n",
        "              f\"{len(self.uri2text)} texts, \",\n",
        "              f\"{len(self.uri2word_ali)} word alinments, \",\n",
        "             f\"{len(self.uri2phone_ali)} phone alignments\")\n",
        "\n",
        "    def get_uri(self, index_or_uri: Union[str, int]):\n",
        "        if isinstance(index_or_uri, str):\n",
        "            uri = index_or_uri\n",
        "        else:\n",
        "            uri = self.uris[index_or_uri]\n",
        "        return uri\n",
        "\n",
        "\n",
        "    def get_audio(self, index_or_uri: Union[str, int]):\n",
        "        uri = self.get_uri(index_or_uri)\n",
        "        wav_path = self.uri2wav[uri]\n",
        "        # wav_channels, sr = torchaudio.load(wav_path)\n",
        "        wav_channels, sr = librosa.load(wav_path, sr = None)\n",
        "        return wav_channels, sr\n",
        "\n",
        "    def get_text(self, index_or_uri: Union[str, int]):\n",
        "        \"\"\" Return (start_sample, stop_sample, text)\"\"\"\n",
        "        uri = self.get_uri(index_or_uri)\n",
        "        txt_path = self.uri2text[uri]\n",
        "        with open(txt_path) as f:\n",
        "            start, stop, text = f.read().strip().split(maxsplit=2)\n",
        "            start, stop = int(start), int(stop)\n",
        "            assert start == 0, f\"{txt_path}\"\n",
        "        return start, stop, text\n",
        "\n",
        "    def get_word_ali(self, index_or_uri):\n",
        "        \"\"\" Return [(start_sample, stop_sample, word), ...]\"\"\"\n",
        "        uri = self.get_uri(index_or_uri)\n",
        "        wrd_path = self.uri2word_ali[uri]\n",
        "        with open(wrd_path) as f:\n",
        "            words = [(int(start), int(stop), word) for start, stop, word in map(str.split, f.readlines())]\n",
        "        return words\n",
        "\n",
        "    def get_phone_ali(self, index_or_uri):\n",
        "        \"\"\" Return [(start_sample, stop_sample, phone), ...]\"\"\"\n",
        "        uri = self.get_uri(index_or_uri)\n",
        "        ph_path = self.uri2phone_ali[uri]\n",
        "        with open(ph_path) as f:\n",
        "            phonemes = [(int(start), int(stop), ph) for start, stop, ph in map(str.split, f.readlines())]\n",
        "        return phonemes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {\"uri\": self.get_uri(index),\n",
        "                \"audio\": self.get_audio(index),\n",
        "                \"text\": self.get_text(index),\n",
        "                \"word_ali\": self.get_word_ali(index),\n",
        "                \"phone_ali\": self.get_phone_ali(index)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.uris)\n",
        "\n",
        "    def total_audio_samples(self) -> int:\n",
        "        audio_len = 0\n",
        "        for uri in self.uris:\n",
        "          audio, sr = self.get_audio(uri)\n",
        "          audio_len+=len(audio)\n",
        "        return audio_len\n",
        "\n",
        "    def total_num_words(self) -> int:\n",
        "        num_words = 0\n",
        "        for uri in self.uris:\n",
        "          words = self.get_word_ali(uri)\n",
        "          num_words+=len(words)\n",
        "        return num_words\n",
        "\n",
        "    def total_num_phones(self) -> int:\n",
        "        num_phones = 0\n",
        "        for uri in self.uris:\n",
        "          phones = self.get_phone_ali(uri)\n",
        "          num_phones+=len(phones)\n",
        "        return num_phones\n",
        "\n",
        "    def get_vocab(self) -> Set[str]:\n",
        "        unique_words = set()\n",
        "        for uri in self.uris:\n",
        "          words = self.get_word_ali(uri)\n",
        "          words = [w[2].lower().strip() for w in words]\n",
        "          unique_words.update(words)\n",
        "        return unique_words\n",
        "\n",
        "    def get_phones(self) -> Set[str]:\n",
        "        unique_phones = set()\n",
        "        for uri in self.uris:\n",
        "          phones = self.get_phone_ali(uri)\n",
        "          phones = [p[2].lower().strip() for p in phones]\n",
        "          unique_phones.update(phones)\n",
        "        return unique_phones\n",
        "\n",
        "    def phones_prior(self) -> Dict[str, float]:\n",
        "        all_phones = list()\n",
        "        for uri in self.uris:\n",
        "          phones = self.get_phone_ali(uri)\n",
        "          phones = [p[2].lower().strip() for p in phones]\n",
        "          all_phones.extend(phones)\n",
        "\n",
        "        counts = Counter(all_phones)\n",
        "        total_count = counts.total()\n",
        "        counts = {k: v/total_count for k,v in dict(counts).items()}\n",
        "        return counts\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatsDataset(TimitDataset):\n",
        "    def __init__(self, data_path, feature_extractor, tokenizer, sr = 16000):\n",
        "        super().__init__(data_path)\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sr = sr\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        orig_item = super().__getitem__(index)\n",
        "        wav, sr = orig_item['audio']\n",
        "        text = orig_item['text'][2]\n",
        "\n",
        "        return {\n",
        "                \"wav\": wav,\n",
        "                \"sr\": sr,\n",
        "                \"text\": text\n",
        "                }\n",
        "\n",
        "    def collate_pad(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        batch_texts = [d['text'] for d in batch]\n",
        "        batch_wavs = [d['wav'] for d in batch]\n",
        "        sr = self.sr\n",
        "\n",
        "        labels = self.tokenizer(batch_texts, return_tensors=\"pt\", padding=True).input_ids\n",
        "        feats = self.feature_extractor(batch_wavs, sampling_rate=sr, return_tensors=\"pt\", padding='longest').input_values\n",
        "\n",
        "        return {'input_values': feats,\n",
        "               'labels': labels,\n",
        "               }\n"
      ],
      "metadata": {
        "id": "ba-fp4Fy2VOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "EMr8H5NO6VB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "encoder_id = \"facebook/wav2vec2-base-960h\"  # acoustic model encoder\n",
        "decoder_id = \"google-bert/bert-base-uncased\"  # text decoder"
      ],
      "metadata": {
        "id": "SMTIu92Yy6Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7L7F0--0ta1",
        "outputId": "7de9f354-2b36-4f15-f1ec-776b3de25a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(decoder_id)"
      ],
      "metadata": {
        "id": "cdaEIHP14xjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = FeatsDataset('timit/data/TEST/', feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
        "train_ds = FeatsDataset('timit/data/TRAIN/', feature_extractor=feature_extractor, tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzP55I0f45Gk",
        "outputId": "59e54164-30d8-4ef7-b9e8-1665ae8c24b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1680 utterances in timit/data/TEST/.  1680 wavs,  1680 texts,  1680 word alinments,  1680 phone alignments\n",
            "Found 4620 utterances in timit/data/TRAIN/.  4620 wavs,  4620 texts,  4620 word alinments,  4620 phone alignments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)\n",
        "\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBmP1qiEFTlR",
        "outputId": "b9c25167-289c-46a1-b387-81b14d2e376b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set random seed for reproducibility\n",
        "import random\n",
        "import torch\n",
        "from transformers.file_utils import is_torch_available\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if is_torch_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "gJsJVP88HUgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "wer_metric = load(\"wer\")\n",
        "\n",
        "class WER():\n",
        "  def __init__(self):\n",
        "    self.wer_batched = []\n",
        "\n",
        "  def __call__(self, pred, compute_result):\n",
        "    if compute_result:\n",
        "      result = {'wer':np.mean(self.wer_batched)}\n",
        "      self.wer_batched = []\n",
        "    else:\n",
        "      wer = self.compute_metrics(pred)\n",
        "      self.wer_batched.append(wer)\n",
        "      result = {'wer':wer}\n",
        "    return result\n",
        "\n",
        "  def compute_metrics(self, pred):\n",
        "      pred_ids = torch.argmax(pred.predictions[0],dim=2)\n",
        "      label_ids = pred.label_ids\n",
        "\n",
        "      pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "      label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "      wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "      return wer\n"
      ],
      "metadata": {
        "id": "ROZ7H9vb7yy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EarlyStoppingCallback, TrainingArguments, Trainer\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"AED_from_pretrained\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    # eval_strategy=\"steps\",\n",
        "    # eval_steps= 10,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps = 50,\n",
        "    save_total_limit = 1,\n",
        "    metric_for_best_model='wer',\n",
        "    greater_is_better=False,\n",
        "    dataloader_drop_last=True,\n",
        "    load_best_model_at_end=True,\n",
        "    remove_unused_columns = False,\n",
        "    batch_eval_metrics = True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    data_collator=train_ds.collate_pad,\n",
        "    compute_metrics=WER(),\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "LKS1rdMqzGcd",
        "outputId": "1d06e7aa-0fb1-4b68-9662-104c629176e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msaltandmatches\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241214_224548-95jmw2y1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/saltandmatches/huggingface/runs/95jmw2y1' target=\"_blank\">AED_from_pretrained</a></strong> to <a href='https://wandb.ai/saltandmatches/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/saltandmatches/huggingface' target=\"_blank\">https://wandb.ai/saltandmatches/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/saltandmatches/huggingface/runs/95jmw2y1' target=\"_blank\">https://wandb.ai/saltandmatches/huggingface/runs/95jmw2y1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2308' max='5770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2308/5770 30:49 < 46:16, 1.25 it/s, Epoch 4/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.325800</td>\n",
              "      <td>2.836602</td>\n",
              "      <td>0.681021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.270800</td>\n",
              "      <td>2.878866</td>\n",
              "      <td>0.657160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.013100</td>\n",
              "      <td>2.849786</td>\n",
              "      <td>0.641864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.857200</td>\n",
              "      <td>2.846958</td>\n",
              "      <td>0.637151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['decoder.cls.predictions.decoder.weight', 'decoder.cls.predictions.decoder.bias'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2308, training_loss=1.8007373619740716, metrics={'train_runtime': 1854.2275, 'train_samples_per_second': 24.916, 'train_steps_per_second': 3.112, 'total_flos': 0.0, 'train_loss': 1.8007373619740716, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9k08MtHN0Zt9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}